{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Info\" data-toc-modified-id=\"Info-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Info</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Preliminary\" data-toc-modified-id=\"Preliminary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preliminary</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#All-data-missing-per-row-distribution\" data-toc-modified-id=\"All-data-missing-per-row-distribution-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>All data missing per row distribution</a></span></li><li><span><a href=\"#Number-of-available-variables-per-row\" data-toc-modified-id=\"Number-of-available-variables-per-row-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Number of available variables per row</a></span></li><li><span><a href=\"#Number-of-available-variables-per-row-for-defined-outcome\" data-toc-modified-id=\"Number-of-available-variables-per-row-for-defined-outcome-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Number of available variables per row for defined outcome</a></span></li><li><span><a href=\"#Number-of-available-variables-per-row-for-reduced-dataset\" data-toc-modified-id=\"Number-of-available-variables-per-row-for-reduced-dataset-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>Number of available variables per row for reduced dataset</a></span></li></ul></li><li><span><a href=\"#Class-balance\" data-toc-modified-id=\"Class-balance-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Class balance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Before-removing-row-with-too-many-missing\" data-toc-modified-id=\"Before-removing-row-with-too-many-missing-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Before removing row with too many missing</a></span></li><li><span><a href=\"#After-removing-row-with-too-many-missing\" data-toc-modified-id=\"After-removing-row-with-too-many-missing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>After removing row with too many missing</a></span></li></ul></li><li><span><a href=\"#Features-(left—raw,-right—preprocessed-or-removed)\" data-toc-modified-id=\"Features-(left—raw,-right—preprocessed-or-removed)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Features (left—raw, right—preprocessed or removed)</a></span></li><li><span><a href=\"#Outcome-distribution\" data-toc-modified-id=\"Outcome-distribution-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Outcome distribution</a></span></li><li><span><a href=\"#Missing-data\" data-toc-modified-id=\"Missing-data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Missing data</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Machine-learning\" data-toc-modified-id=\"Machine-learning-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Machine learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Default\" data-toc-modified-id=\"Default-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Default</a></span></li><li><span><a href=\"#TODO:-Optimized\" data-toc-modified-id=\"TODO:-Optimized-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>TODO: Optimized</a></span></li></ul></li><li><span><a href=\"#SRTR—Work-in-progress\" data-toc-modified-id=\"SRTR—Work-in-progress-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>SRTR—Work in progress</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#SRTR\" data-toc-modified-id=\"SRTR-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>SRTR</a></span></li><li><span><a href=\"#ML\" data-toc-modified-id=\"ML-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>ML</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "\n",
    "for folder in itertools.chain([Path.cwd()], Path.cwd().parents):\n",
    "    if (folder / 'Pipfile').exists():\n",
    "        os.chdir(folder)\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext rpy2.ipython\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from utils import Breakpoint, generate_type_variants\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from custom_types import Estimator\n",
    "from notebooks.heart_transplant.heart_transplant_functions import log_transform, filter_out_unused_features, get_X_y_1_year_survival\n",
    "\n",
    "import pandas\n",
    "\n",
    "dataset_raw = pandas.read_csv(\n",
    "    \"../cardiovascular-risk-data/data/UNOS.csv\",\n",
    "    dtype={\n",
    "        'func_stat_tcr': 'str',\n",
    "    }\n",
    ")\n",
    "dataset_raw.columns = [column.lower() for column in dataset_raw.columns]\n",
    "pandas.set_option('display.max_columns', None)\n",
    "\n",
    "from functional import pipe\n",
    "from functools import partial\n",
    "from heart_transplant_data import heart_transplant_metadata as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formatting import format_percents\n",
    "from heart_transplant_functions import extract_y\n",
    "\n",
    "n_y_not_defined = extract_y(dataset_raw).isna().value_counts().get(True, 0)\n",
    "\n",
    "print(f'Outcome not defined: {format_percents(n_y_not_defined/len(dataset_raw))} ({n_y_not_defined})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import pipe\n",
    "from functools import partial\n",
    "from heart_transplant_data import heart_transplant_metadata as metadata\n",
    "from toolz import compose\n",
    "\n",
    "\n",
    "features = list(metadata.keys())\n",
    "\n",
    "def feature_engeneering(dataset):\n",
    "    dataset_new = dataset.copy()\n",
    "    # TODO: parse\n",
    "    dataset_new.drop('func_stat_tcr', axis=1, inplace=True)\n",
    "    return dataset_new\n",
    "\n",
    "def type_conversion(dataset):\n",
    "    dataset_new = dataset.copy()\n",
    "    dataset_new['height ratio'] = pandas.to_numeric(dataset['height ratio'])\n",
    "    dataset_new['weight ratio'] = pandas.to_numeric(dataset['weight ratio'])\n",
    "    return dataset_new\n",
    "\n",
    "def set_na(_X):\n",
    "    _X_new = _X.copy()\n",
    "    for column in _X.columns:\n",
    "        metadata_record = metadata[column]\n",
    "        try:\n",
    "            metadata_record['na_values']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            _X_new[column].replace(metadata_record['na_values'], np.nan, inplace=True)\n",
    "\n",
    "    return _X_new\n",
    "\n",
    "\n",
    "dataset_transform_base = compose(\n",
    "    type_conversion,\n",
    "    feature_engeneering,\n",
    "    set_na,\n",
    "    partial(filter_out_unused_features, metadata=metadata),\n",
    ")\n",
    "\n",
    "dataset_without_log = dataset_transform_base(dataset_raw)\n",
    "\n",
    "dataset = log_transform(dataset_without_log)\n",
    "\n",
    "X, y = get_X_y_1_year_survival(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of available variable per row histogram (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw.apply(lambda x: x.count(), axis=1).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of available variables per row histogram (after preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_without_log.apply(lambda x: x.count(), axis=1).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of available variables per row distribution (removed rows with undefined outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.apply(lambda x: x.count(), axis=1).hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = X.apply(lambda x: x.count() >= 80, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X[missing_mask]\n",
    "y_reduced = y[missing_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total rows raw**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total rows with defined 1 year survival**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total rows after removing rows with too many missing variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Percentage of removed rows due to too many missing variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_percents(1-(len(X_reduced)/len(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of available variables per row distributin (removed rows with too many missing variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced.apply(lambda x: x.count(), axis=1).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before removing row with too many missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After removing row with too many missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reduced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_reduced.value_counts()\n",
    "class_ratio = class_counts[0]/class_counts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Features (left—raw, right—preprocessed or removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from formatting import b\n",
    "from heart_transplant_data import heart_transplant_metadata as metadata\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "for index, (feature_name, series) in enumerate(dataset_raw.items()):\n",
    "    metadata_item = metadata.get(feature_name)\n",
    "    fig, ax = pyplot.subplots(1, 2)\n",
    "    \n",
    "    try:\n",
    "        display_name = f'{metadata_item[\"name_long\"]} ({feature_name})'\n",
    "    except (KeyError, TypeError):\n",
    "        display_name = f'{feature_name}'\n",
    "    \n",
    "    try:\n",
    "        series.sample(frac=0.1).hist(grid=False, ax=ax[0])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        X_reduced[feature_name]\n",
    "    except KeyError:\n",
    "        ax[1].axis(\"off\")\n",
    "    else:\n",
    "        dataset_without_log[feature_name].loc[X_reduced.index].sample(frac=0.1).hist(\n",
    "            grid=False, ax=ax[1]\n",
    "        )\n",
    "    fig.suptitle(f'{index+1}. {display_name}')\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw['death'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "missingno.matrix(dataset_raw)\n",
    "pyplot.show()\n",
    "missingno.matrix(X)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgrid\n",
    "\n",
    "sg = partial(qgrid.show_grid, grid_options={'forceFitColumns': False, 'defaultColumnWidth': 200}, show_toolbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- cmv_igg PD C (strange values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "results_per_method = {}\n",
    "cache_directory = mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nested_cv import StaticHyperParameters, SimpleEvaluationProtocol\n",
    "from sklearn.impute import SimpleImputer\n",
    "from nested_cv import evaluate_simple_cross_validation\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from utils import use_df\n",
    "from xgboost import XGBClassifier\n",
    "from utils import evaluate_and_assign_if_not_present\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from toolz import valmap\n",
    "from nested_cv import get_cv_results_from_simple_cv_evaluation\n",
    "from functools import partial\n",
    "from functional import pipe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from evaluation_functions import compute_classification_metrics_from_results_with_statistics\n",
    "set_config(display='diagram')\n",
    "\n",
    "X, y = get_X_y_1_year_survival(dataset)\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(pandas.DataFrame(X).head())\n",
    "        print(X.shape)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "# for feature in categorical_features:\n",
    "#     X[feature]=X[feature].astype('category')\n",
    "\n",
    "\n",
    "categorical_features = [feature_name for feature_name, series in X.items() if (series.dtype == 'object' or feature_name in [\n",
    "    'iabp_tcr', 'inotropes_tcr', 'ethcat', 'lvad ever', 'gender', 'gender_don', 'tah ever', 'med_cond_trr', 'ecmo_trr', 'gstatus',\n",
    "    'pstatus', 'ethcat_don', 'blood_inf_don', 'other_inf_don', 'pulm_inf_don', 'urine_inf_don', 'cod_cad_don',\n",
    "    'death_mech_don', 'multiorg', 'abo_mat', 'lv_eject_meth', 'coronary_angio', 'vessels_50sten', 'biopsy_dgn',\n",
    "    'ecd_donor', 'education', 'education', 'congenital', 'prior_card_surg_type_tcr', 'ventilator_tcr', 'rvad ever', 'cancer_site_don',\n",
    "])]\n",
    "\n",
    "continuous_features = [\n",
    "    feature_name for feature_name in X.columns if feature_name not in categorical_features\n",
    "]\n",
    "\n",
    "\n",
    "keys = [ X[column].unique() for column in categorical_features ]\n",
    "feature_names = [*categorical_features, *continuous_features]\n",
    "\n",
    "transform_pipeline = make_pipeline(\n",
    "    use_df(ColumnTransformer, columns=feature_names)([\n",
    "         (\n",
    "             'categorical',\n",
    "             make_pipeline(\n",
    "                 SimpleImputer(strategy='most_frequent'),\n",
    "                 OrdinalEncoder(categories=keys)\n",
    "             ),\n",
    "             categorical_features,\n",
    "         ),\n",
    "         ('continuous', SimpleImputer(strategy='mean'), continuous_features),\n",
    "     ]),\n",
    ")\n",
    "\n",
    "\n",
    "evaluate_and_assign_if_not_present(\n",
    "    results_per_method,\n",
    "    'xgboost_balanced',\n",
    "    lambda: evaluate_simple_cross_validation(\n",
    "        lambda: make_pipeline(\n",
    "            transform_pipeline,\n",
    "            StandardScaler(),\n",
    "            use_df(XGBClassifier)(\n",
    "            eval_metric='logloss',\n",
    "                scale_pos_weight=class_ratio,\n",
    "                use_label_encoder=False,\n",
    "                n_jobs=12,\n",
    "            )\n",
    "        ),\n",
    "        X_reduced,\n",
    "        y_reduced,\n",
    "        StaticHyperParameters({}),\n",
    "        SimpleEvaluationProtocol(repeats=1, cv=10),\n",
    "        parallel=False,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate_and_assign_if_not_present(\n",
    "    results_per_method,\n",
    "    'xgboost',\n",
    "    lambda: evaluate_simple_cross_validation(\n",
    "        lambda: make_pipeline(\n",
    "            transform_pipeline,\n",
    "            StandardScaler(),\n",
    "            use_df(XGBClassifier)(\n",
    "            eval_metric='logloss',\n",
    "                use_label_encoder=False,\n",
    "                n_jobs=12,\n",
    "            )\n",
    "        ),\n",
    "        X_reduced,\n",
    "        y_reduced,\n",
    "        StaticHyperParameters({}),\n",
    "        SimpleEvaluationProtocol(repeats=1, cv=10),\n",
    "        parallel=False,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate_and_assign_if_not_present(\n",
    "    results_per_method,\n",
    "    'random_forest',\n",
    "    lambda: evaluate_simple_cross_validation(\n",
    "        lambda: make_pipeline(\n",
    "            transform_pipeline,\n",
    "            StandardScaler(),\n",
    "            use_df(RandomForestClassifier)()\n",
    "        ),\n",
    "        X_reduced,\n",
    "        y_reduced,\n",
    "        StaticHyperParameters({}),\n",
    "        SimpleEvaluationProtocol(repeats=1, cv=10),\n",
    "        parallel=False,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate_and_assign_if_not_present(\n",
    "    results_per_method,\n",
    "    'logistic_regression',\n",
    "    lambda: evaluate_simple_cross_validation(\n",
    "        lambda: make_pipeline(\n",
    "            transform_pipeline,\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(),\n",
    "        ),\n",
    "        X_reduced,\n",
    "        y_reduced,\n",
    "        StaticHyperParameters({}),\n",
    "        SimpleEvaluationProtocol(repeats=1, cv=10),\n",
    "        parallel=False,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "metrics_per_method = pipe(\n",
    "    results_per_method,\n",
    "    partial(valmap, get_cv_results_from_simple_cv_evaluation),\n",
    "    partial(valmap, partial(compute_classification_metrics_from_results_with_statistics, y))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate_and_assign_if_not_present(\n",
    "    results_per_method,\n",
    "    'xgboost_balanced_optimized',\n",
    "    lambda: evaluate_simple_cross_validation(\n",
    "        lambda: make_pipeline(\n",
    "            transform_pipeline,\n",
    "            StandardScaler(),\n",
    "            use_df(XGBClassifier)(\n",
    "            eval_metric='logloss',\n",
    "                scale_pos_weight=class_ratio,\n",
    "                use_label_encoder=False,\n",
    "                n_jobs=12,\n",
    "            )\n",
    "        ),\n",
    "        X_reduced,\n",
    "        y_reduced,\n",
    "        BayesianOptimization(XGBoostMethod.get_hyperopt_space()),\n",
    "        SimpleEvaluationProtocol(repeats=1, cv=10),\n",
    "        parallel=False,\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRTR—Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart_transplant_functions import reverse_log_transform_row\n",
    "from pandas import Series\n",
    "\n",
    "\n",
    "def predict_srtr(row: Series) -> float:\n",
    "    row_raw = reverse_log_transform_row(row)\n",
    "    outcome = 0.\n",
    "    \n",
    "    if row['diab'] == 1:\n",
    "        outcome += -0.066605837\n",
    "    elif pandas.isna(row['diab']):\n",
    "        outcome += -0.066605837\n",
    "        \n",
    "    if row['education'] == 4:\n",
    "        outcome += -0.000272643\n",
    "    elif pandas.isna(row['education']):\n",
    "        outcome += -0.000272643\n",
    "        \n",
    "    if row['cig_use'] == 'Y':\n",
    "        outcome += 0.065904842\n",
    "    \n",
    "    if row['malig'] == 'Y':\n",
    "        outcome += 0.02219013\n",
    "        \n",
    "    if row['ethcat'] == 2:\n",
    "        outcome += -0.088669408\n",
    "    elif row['ethcat'] == 7:\n",
    "        outcome += -0.434641386\n",
    "    elif row['ethcat'] == 1:\n",
    "        outcome += 0.016172921\n",
    "    elif pandas.isna(row['ethcat']):\n",
    "        outcome += -0.434641386\n",
    "    \n",
    "    if row['hist_oth_drug_don'] == 'N':\n",
    "        outcome += 0.003011283\n",
    "        \n",
    "    if row['hist_cig_don'] == 'Y':\n",
    "        outcome += 0.000498645\n",
    "    \n",
    "    if row_raw['diag'] == 1201 or row_raw['diag'] == 1208:\n",
    "        outcome += -0.11598044\n",
    "    elif pandas.isna(row_raw['diag']):\n",
    "        outcome += -0.11598044\n",
    "    \n",
    "    if row_raw['age_don'] > 55:\n",
    "        outcome += 0.022276125\n",
    "    elif row_raw['age_don'] > 50:\n",
    "        outcome += 0.024025803\n",
    "    elif row_raw['age_don'] > 40:\n",
    "        outcome += 0.024025803\n",
    "    elif row_raw['age_don'] > 20:\n",
    "        outcome += 0.001105558\n",
    "    elif row_raw['age_don'] > 15:\n",
    "        outcome += 0.0000457    \n",
    "        \n",
    "        \n",
    "    if row['hist_hypertens_don'] == 'Y':\n",
    "        outcome += 0.000140302\n",
    "        \n",
    "    if row_raw['bmi_don_calc'] < 22:\n",
    "        outcome += -0.013675144\n",
    "    elif pandas.isna(row_raw['bmi_don_calc']):\n",
    "        outcome += -0.054750638\n",
    "    \n",
    "    \n",
    "    if row_raw['bun_don'] < 10:\n",
    "        outcome += 0.006214231\n",
    "    elif row_raw['bun_don'] < 15:\n",
    "        outcome += 0.030163193\n",
    "    elif row_raw['bun_don'] < 25:\n",
    "        outcome += 0.000675563\n",
    "        \n",
    "    #TODO: We have values 1-4, mapped to the parameters from excel file\n",
    "    # Cerebrovascular/Stroke\n",
    "    if row['cod_cad_don'] == 2:\n",
    "        outcome += 0.009943402\n",
    "    \n",
    "    if row['gender_don'] == 'F':\n",
    "        outcome += 0.131377454\n",
    "    \n",
    "    if row['donor insulin'] == 'Y':\n",
    "        outcome += -0.243727342\n",
    "    elif pandas.isna(row['donor insulin']):\n",
    "        outcome += -0.243727342\n",
    "    \n",
    "    if row['ethcat_don'] == 9:\n",
    "        outcome += -0.005654728\n",
    "    elif row['ethcat_don'] == 7:\n",
    "        outcome += -0.022069858\n",
    "    elif row['ethcat_don'] == 1:\n",
    "        outcome += 0.007700082\n",
    "    elif pandas.isna(row['ethcat_don']):\n",
    "        outcome += -0.022069858\n",
    "    \n",
    "    if row_raw['height ratio'] > 1.08:\n",
    "        outcome += 0.123483738\n",
    "    elif row_raw['height ratio'] > 1.06:\n",
    "        outcome += 0.177597673     \n",
    "    elif row_raw['height ratio'] > 0.98:\n",
    "        outcome += 0.177597673\n",
    "        \n",
    "    if row_raw['weight ratio'] < 0.7:\n",
    "        outcome += -0.676215593\n",
    "    elif row_raw['weight ratio'] > 1.6:\n",
    "        outcome += -0.616706456\n",
    "    elif row_raw['weight ratio'] > 1.3:\n",
    "        outcome += -0.333586524\n",
    "    elif pandas.isna(row_raw['weight ratio']):\n",
    "        outcome += -0.32244479\n",
    "    \n",
    "    if row_raw['creat_don'] < 0.8:\n",
    "        outcome += -0.0000895\n",
    "    elif pandas.isna(row_raw['creat_don']): \n",
    "        outcome += -0.003848936\n",
    "\n",
    "    if row_raw['age'] < 70:\n",
    "        outcome += -0.0048657\n",
    "    elif row_raw['age'] > 40:\n",
    "        outcome += 0.011148729\n",
    "    elif pandas.isna(row_raw['age']):\n",
    "        outcome += -0.240346908\n",
    "    \n",
    "    if row_raw['bmi_calc'] > 36:\n",
    "        outcome += 0.156816331\n",
    "    elif row_raw['bmi_calc'] > 34:\n",
    "        outcome += 0.015018691\n",
    "    elif row_raw['bmi_calc'] > 30:\n",
    "        outcome += 0.002490638\n",
    "    \n",
    "    if row_raw['hemo_co_tcr'] > 3.5:\n",
    "        outcome += -0.000561188\n",
    "    elif pandas.isna(row_raw['hemo_co_tcr']):\n",
    "        outcome += -0.002749823\n",
    "    \n",
    "    if row_raw['most_rcnt_creat'] < 0.7:\n",
    "        outcome += 1.751207911\n",
    "    elif row_raw['most_rcnt_creat'] > 1.5:\n",
    "        outcome += 0.161043865\n",
    "    elif row_raw['most_rcnt_creat'] > 1:\n",
    "        outcome += 0.355230656\n",
    "    elif row_raw['most_rcnt_creat'] > 0.9:\n",
    "        outcome += 0.004575029\n",
    "    \n",
    "    if row_raw['dial_after_list'] == 'Y':\n",
    "        outcome += 0.484377854\n",
    "    \n",
    "    if row_raw['ecmo_trr'] == 1:\n",
    "        outcome += 0.350887035\n",
    "    \n",
    "    # TODO ischemic time, correct variable?\n",
    "    if row_raw['ischtime'] < 120:\n",
    "        outcome += -0.00088359\n",
    "    elif row_raw['ischtime'] < 140:\n",
    "        outcome += -0.000286891\n",
    "    elif row_raw['ischtime'] < 160:\n",
    "        outcome += -0.0000401  \n",
    "    elif row_raw['ischtime'] < 300:\n",
    "        outcome += 0.000587559\n",
    "    \n",
    "    if pandas.isna(row_raw['ischtime']):\n",
    "        outcome += -0.218530997\n",
    "        \n",
    "    if row_raw['med_cond_trr'] != 'ICU':\n",
    "        outcome += -0.075556547\n",
    "        \n",
    "    if row_raw['hemo_pcw_tcr'] < 16:\n",
    "        outcome += 0.001151401\n",
    "    elif row_raw['hemo_pcw_tcr'] < 18:\n",
    "        outcome += 0.000863272\n",
    "    elif row_raw['hemo_pcw_tcr'] > 32:\n",
    "        outcome += 0.000288132\n",
    "    elif row_raw['hemo_pcw_tcr'] > 30:\n",
    "        outcome += 0.016847899\n",
    "    elif row_raw['hemo_pcw_tcr'] > 28:\n",
    "        outcome += 0.013182796\n",
    "    \n",
    "    if row_raw['retransplant'] >= 1:\n",
    "        outcome += 0.105895416\n",
    "        \n",
    "    if row_raw['hemo_pa_dia_tcr'] > 32:\n",
    "        outcome += 0.003828561\n",
    "    elif row_raw['hemo_pa_dia_tcr'] > 30:\n",
    "        outcome += 0.00102561\n",
    "        \n",
    "    if row_raw['hemo_sys_tcr'] < 20:\n",
    "        outcome += 0.002339633\n",
    "    elif row_raw['hemo_pa_dia_tcr'] < 25:\n",
    "        outcome += 0.003705285\n",
    "    elif row_raw['hemo_pa_dia_tcr'] > 65:\n",
    "        outcome += 0.002035943\n",
    "    \n",
    "    if row_raw['rvad ever'] == 1:\n",
    "        outcome += 0.025412411\n",
    "\n",
    "    if row_raw['tah ever'] == 1:\n",
    "        outcome += 0.740901872\n",
    "\n",
    "    if row_raw['tbili'] > 2:\n",
    "        outcome += 0.29652491\n",
    "    elif row_raw['tbili'] > 0.4:\n",
    "        outcome += 0.199733923\n",
    "    \n",
    "    if row_raw['transfusions'] == 'Y':\n",
    "        outcome += 0.289314026\n",
    "    \n",
    "    if row_raw['ventilator_tcr'] == 1:\n",
    "        outcome += 0.289314026\n",
    "    \n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_srtr = X.apply(predict_srtr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inverse_binary_y\n",
    "\n",
    "d = pandas.DataFrame({\n",
    "    'y': inverse_binary_y(y),\n",
    "    'y_pred_srtr': y_pred_srtr\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(inverse_binary_y(y), y_pred_srtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation import list_of_lists_to_html_table, display_html\n",
    "from formatting import compare_metrics_in_table\n",
    "\n",
    "display_html(list_of_lists_to_html_table(compare_metrics_in_table(metrics_per_method)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
